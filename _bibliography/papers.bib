---
---

@string{aps = {American Physical Society,}}


@article{klmtneurips,
  abbr={Conference},
  title={Fundamental Convergence Analysis of Sharpness-Aware Minimization},
  author={Khanh, P. D. and Luong, H-C and Mordukhovich, B. S. and Tran, D. B.},
  abstract={The paper investigates the fundamental convergence properties of Sharpness-Aware Minimization (SAM), a recently proposed gradient-based optimization method (Foret et al., 2021) that significantly improves the generalization of deep neural networks. The convergence properties including the stationarity of accumulation points, the convergence of the sequence of gradients to the origin, the sequence of function values to the optimal value, and the sequence of iterates to the optimal solution are established for the method. The universality of the provided convergence analysis based on inexact gradient descent frameworks (Khanh et al., 2023b) allows its extensions to the normalized versions of SAM such as VaSSO (Li & Giannakis, 2023), RSAM (Liu et al., 2022), and to the unnormalized versions of SAM such as USAM (Andriushchenko & Flammarion, 2022). Numerical experiments are conducted on classification tasks using deep learning models to confirm the practical aspects of our analysis.},
  journal={Advances in Neural Information Processing Systems},
  year={2024},
  publisher=aps,
  url={https://arxiv.org/abs/2401.08060},
  html={https://neurips.cc/},
  altmetric={},
  dimensions={true},
  google_scholar_id={iH-uZ7U-co4C},
  selected={true},
  bibtex_show={true},
}
@article{kmptMAPR,
  abbr={Journal},
  title={Globally convergent coderivative-based generalized Newton methods in nonsmooth optimization},
  abstract={This paper proposes and justifies two globally convergent Newton-type methods to solve unconstrained and constrained problems of nonsmooth optimization by using tools of variational analysis and generalized differentiation. Both methods are coderivative-based and employ generalized Hessians (coderivatives of subgradient mappings) associated with objective functions, which are either of class C1,1, or are represented in the form of convex composite optimization, where one of the terms may be extended-real-valued. The proposed globally convergent algorithms are of two types. The first one extends the damped Newton method and requires positive-definiteness of the generalized Hessians for its well-posedness and efficient performance, while the other algorithm is of {the regularized Newton type} being well-defined when the generalized Hessians are merely positive-semidefinite. The obtained convergence rates for both methods are at least linear, but become superlinear under the semismooth∗ property of subgradient mappings. Problems of convex composite optimization are investigated with and without the strong convexity assumption {on smooth parts} of objective functions by implementing the machinery of forward-backward envelopes. Numerical experiments are conducted for Lasso problems and for box constrained quadratic programs with providing performance comparisons of the new algorithms and some other first-order and second-order methods that are highly recognized in nonsmooth optimization.},
  author={Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T and Tran, D. B.},
  journal={Mathematical Programming},
  volume={205},
  pages={373-429},
  year={2024},
  doi={10.1007/s10107-023-01980-2},
  publisher=Springer,
  html={https://link.springer.com/article/10.1007/s10107-023-01980-2},
  dimensions={true},
  google_scholar_id={Zph67rFs4hoC},
  selected={true},
  bibtex_show={true},
}

@article{kmptJOGO,
  abbr={Journal},
  title={Generalized damped Newton algorithms in nonsmooth optimization via second-order subdifferentials},
  abstract={The paper proposes and develops new globally convergent algorithms of the generalized damped Newton type for solving important classes of nonsmooth optimization problems. These algorithms are based on the theory and calculations of second-order subdifferentials of nonsmooth functions with employing the machinery of second-order variational analysis and generalized differentiation. First we develop a globally superlinearly convergent damped Newton-type algorithm for the class of continuously differentiable functions with Lipschitzian gradients, which are nonsmooth of second order. Then we design such a globally convergent algorithm to solve a structured class of nonsmooth quadratic composite problems with extended-real-valued cost functions, which typically arise in machine learning and statistics. Finally, we present the results of numerical experiments and compare the performance of our main algorithm applied to an important class of Lasso problems with those achieved by other first-order and second-order optimization algorithms.},
  author={Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T and Tran, D. B.},
  journal={Journal of Global Optimization},
  volume={86},
  pages={93-122},
  year={2023},
  bibtex_show={true},
  doi={10.1007/s10898-022-01248-7},
  publisher=Springer,
  html={https://link.springer.com/article/10.1007/s10898-022-01248-7},
  dimensions={true},
  pdf={NewtonJOGO.pdf},
  google_scholar_id={YOwf2qJgpHMC},
  bibtex_show={true},
}

@article{kmtJOTA,
  abbr={Journal},
  title={Inexact reduced gradient methods in nonconvex optimization},
  abstract={This paper proposes and develops new linesearch methods with inexact gradient information for finding stationary points of nonconvex continuously differentiable functions on finite-dimensional spaces. Some abstract convergence results for a broad class of linesearch methods are stablished. A general scheme for inexact reduced gradient (IRG) methods is proposed, where the errors in the gradient approximation automatically adapt with the magnitudes of the exact gradients. The sequences of iterations are shown to obtain stationary accumulation points when different stepsize selections are employed. Convergence results with constructive convergence rates for the developed IRG methods are established under the Kurdyka- Lojasiewicz property. The obtained results for the IRG methods are confirmed by encouraging numerical experiments, which demonstrate advantages of automatically controlled errors in IRG methods over other frequently used error selections.},
  author={Khanh, P. D. and Mordukhovich, B. S. and Tran, D. B.},
  journal={Journal of Optimization Theory and Applications},
  pages={1-41},
  year={2023},
  doi={10.1007/s10957-023-02319-9},
  publisher=Springer,
  html={https://link.springer.com/article/10.1007/s10957-023-02319-9},
  dimensions={true},
  pdf={IRG.pdf},
  google_scholar_id={9ZlFYXVOiuMC},
  selected={false},
  bibtex_show={true},
}

@article{kmtOMS,
  abbr={Journal},
  title={A new inexact gradient descent method with applications to nonsmooth convex optimization},
  author={Khanh, P. D. and Mordukhovich, B. S. and Tran, D. B.},
  journal={Optimization Methods and Software},
  abstract={The paper proposes and develops a novel inexact gradient method (IGD) for minimizing C1-smooth functions with Lipschitzian gradients, i.e., for problems of C1,1 optimization. We show that the sequence of gradients generated by IGD converges to zero. The convergence of iterates to stationary points is guaranteed under the Kurdyka- Lojasiewicz (KL) property of the objective function with convergence rates depending on the KL exponent. The newly developed IGD is applied to designing two novel gradient-based methods of nonsmooth convex optimization such as the inexact proximal point methods (GIPPM) and the inexact augmented Lagrangian method (GIALM) for convex programs with linear equality constraints. These two methods inherit global convergence properties from IGD and are confirmed by numerical experiments to have practical advantages over some well-known algorithms of nonsmooth convex optimization.},
  html={https://www.tandfonline.com/doi/full/10.1080/10556788.2024.2322700},
  pdf={OMS.pdf},
  pages={1-29},
  year={2024},
  bibtex_show={true},
  doi={10.1080/10556788.2024.2322700},
  dimensions={true},
  google_scholar_id={7PzlFSSx8tAC},
  bibtex_show={true},
}
}

@article{kmptJOGOprox,
  abbr={Journal},
  bibtex_show={true},
  title={Inexact proximal methods for weakly convex functions},
  author={Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T and Tran, D. B.},
  abstract={This paper proposes and develops inexact proximal methods for finding stationary points of the sum of a smooth function and a nonsmooth weakly convex one, where an error is present in the calculation of the proximal mapping of the nonsmooth term. A general framework for finding zeros of a continuous mapping is derived from our previous paper on this subject to establish convergence properties of the inexact proximal point method when the smooth term is vanished and of the inexact proximal gradient method when the smooth term satisfies a descent condition. The inexact proximal point method achieves global convergence with constructive convergence rates when the Moreau envelope of the objective function satisfies the Kurdyka-Lojasiewicz (KL) property. Meanwhile, when the smooth term is twice continuously differentiable with a Lipschitz continuous gradient and a differentiable approximation of the objective function satisfies the KL property, the inexact proximal gradient method achieves the global convergence of iterates with constructive convergence rates.},
  journal={to appear Journal of Global Optimization},
  html={https://arxiv.org/pdf/2307.15596},
  dimensions={true},
  year={2024},
  pdf={proximal.pdf},
  bibtex_show={true},
  dimensions={true},
  google_scholar_id={-f6ydRqryjwC},
  bibtex_show={true},
}

@article{kmtMAPR,
  abbr={Preprint},
  bibtex_show={true},
  title={Globally Convergent Derivative-Free Methods in Nonconvex Optimization with and without Noise},
  abstract={This paper addresses the study of nonconvex derivative-free optimization problems, where only information of either smooth objective functions or their noisy approximations is available. General derivative-free methods are proposed for minimizing differentiable (not necessarily convex) functions with globally Lipschitz continuous gradients, where the accuracy of approximate gradients is interacting with stepsizes and exact gradient values. Analysis in the noiseless case guarantees convergence of the gradient sequence to the origin as well as global convergence with constructive convergence rates of the sequence of iterates under the Kurdyka-\L ojasiewicz property. In the noisy case, without any noise level information, the designed algorithms reach near-stationary points with providing estimates on the required number of iterations and function evaluations. Addressing functions with locally Lipschitzian gradients, two algorithms are introduced to handle the noiseless and noisy cases, respectively. The noiseless version is based on the standard backtracking linesearch and achieves fundamental convergence properties similarly to the global Lipschitzian case. The noisy version is based on a novel bidirectional linesearch and is shown to reach near-stationary points after a finite number of iterations when the Polyak-\L ojasiewicz inequality is imposed. Numerical experiments are conducted on a diverse set of test problems to demonstrate more robustness of the newly proposed algorithms in comparison with other finite-difference-based schemes and some highly efficient, production-ready codes from the SciPy library.},
  author={Khanh, P. D. and Mordukhovich, B. S. and Tran, D. B.},
  html={https://optimization-online.org/?p=26889},
  url={https://optimization-online.org/?p=26889},
  pdf={Derivative_free_24_07_01.pdf},
  google_scholar_id={M3NEmzRMIkIC},
  year={2024},
  bibtex_show={true},
}
@article{ckmtOptim,
  abbr={Preprint},
  bibtex_show={true},
  title={Local Convergence Analysis for Nonisolated Solutions to Derivative-Free Methods of Optimization},
  abstract={},
  author={Cuong, D. H. and Khanh, P. D. and Mordukhovich, B. S. and Tran, D. B.},
  url={https://optimization-online.org/?p=27216},
  year={2024},
  pdf={Local_derivative_free.pdf},
  google_scholar_id={blknAaTinKkC},
  year={2024},
  bibtex_show={true},
  html={https://optimization-online.org/?p=27216},
  url={https://optimization-online.org/?p=27216},
}

# additional references
@article{kmptSJO1,
  abbr={Journal},
  title={Variational and strong variational convexity in infinite-dimensional variational analysis},
  abstract={},
  author={Khanh, P. D. and Khoa, V. V. H. and Mordukhovich, B. S. and Phat, V. T.},
  journal={SIAM Journal on Optimization},
  volume={34},
  pages={2756-2787},
  year={2024},
  doi={},
  html={},
  dimensions={true},
  pdf={},
  google_scholar_id={_Qo2XoVZTnwC},
  selected={true},
  bibtex_show={true},
}


@article{kmptMonotonicity,
  abbr={Preprint},
  title={Relationships between Global and Local Monotonicity of Operators},
  abstract={},
  author={Khanh, P. D. and Khoa, V. V. H. and Martínez-Legaz, J. E. and Mordukhovich, B. S.},
  journal={arXiv preprint arXiv:2403.20227},
  year={2024},
  doi={},
  html={},
  dimensions={true},
  pdf={},
  google_scholar_id={k_IJM867U9cC},
  selected={false},
  bibtex_show={true},
}

@article{kmptNewtonStructured,
  abbr={Preprint},
  title={Coderivative-Based Newton Methods in Structured Nonconvex and Nonsmooth Optimization},
  abstract={},
  author={Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T.},
  journal={arXiv preprint arXiv:2403.04262},
  year={2024},
  doi={},
  html={},
  dimensions={true},
  pdf={},
  google_scholar_id={isC4tDSrTZIC},
  selected={false},
  bibtex_show={true},
}


@article{kmptSubdiffOptimality,
  abbr={Preprint},
  title={Second-order subdifferential optimality conditions in nonsmooth optimization},
  abstract={},
  author={Khanh, P. D. and Khoa, V. V. H. and Mordukhovich, B. S. and Phat, V. T.},
  journal={arXiv preprint arXiv:2312.16277},
  volume={2},
  year={2023},
  doi={},
  html={},
  dimensions={true},
  pdf={},
  google_scholar_id={j3f4tGmQtD8C},
  selected={false},
  bibtex_show={true},
}

@article{kmptNonconvexFunctions,
  abbr={Preprint},
  title={Local Minimizers of Nonconvex Functions in Banach Spaces via Moreau Envelopes},
  abstract={},
  author={Khanh, P. D. and Khoa, V. V. H. and Mordukhovich, B. S. and Phat, V. T.},
  journal={arXiv preprint arXiv:2311.18586},
  year={2023},
  doi={},
  html={},
  dimensions={true},
  pdf={},
  google_scholar_id={4JMBOYKVnBMC},
  selected={false},
  bibtex_show={true},
}

@article{kmptNewtonSubgradient,
  abbr={Journal},
  title={A generalized Newton method for subgradient systems},
  abstract={},
  author={Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T.},
  journal={Mathematics of Operations Research},
  volume={48},
  pages={1811-1845},
  year={2023},
  doi={},
  html={},
  dimensions={true},
  pdf={},
  google_scholar_id={8k81kl-MbHgC},
  selected={false},
  bibtex_show={true},
}


@article{kmptMaximalMonotonicity,
  abbr={Preprint},
  title={Local maximal monotonicity in variational analysis and optimization},
  abstract={},
  author={Khanh, P. D. and Khoa, V. V. H. and Mordukhovich, B. S. and Phat, V. T.},
  journal={arXiv preprint arXiv:2308.14193},
  volume={5},
  year={2023},
  doi={},
  html={},
  dimensions={true},
  pdf={},
  google_scholar_id={mB3voiENLucC},
  selected={false},
  bibtex_show={true},
}


@article{kmptVariationalSufficiency,
  abbr={Journal},
  title={Variational convexity of functions and variational sufficiency in optimization},
  abstract={},
  author={Khanh, P. D. and Mordukhovich, B. S. and Phat, V. T.},
  journal={SIAM Journal on Optimization},
  volume={33},
  pages={1121-1158},
  year={2023},
  doi={},
  html={},
  dimensions={true},
  pdf={},
  google_scholar_id={4DMP91E08xMC},
  selected={false},
  bibtex_show={true},
}

@article{khanhMoreau2022,
  title={Continuous Fréchet Differentiability of the Moreau Envelope of Convex Functions on Banach Spaces},
  author={Khanh, P. D. and Nguyen, B. T.},
  journal={Journal of Optimization Theory and Applications},
  volume={195},
  pages={1007--1018},
  year={2022},
  selected={false},
  google_scholar_id={aqlVkmm33-oC},
  abbr={Journal}
}

@article{khanhVariational2022,
  title={Variational inequalities governed by strongly pseudomonotone operators},
  author={Kha, P. T. and Khanh, P. D.},
  journal={Optimization},
  volume={71},
  number={7},
  pages={1983--2004},
  year={2022},
  selected={false},
  google_scholar_id={ufrVoPGSRksC},
  abbr={Journal}
}

@inproceedings{khanhConvexity2022,
  title={Variational convexity of functions in Banach spaces},
  author={Khanh, P. D. and Khoa, V. V. H. and Mordukhovich, B. S. and Phat, V. T.},
  booktitle={International Meeting on Functional Analysis and Continuous Optimization},
  year={2022},
  selected={false},
  google_scholar_id={Wp0gIr-vW9MC},
  abbr={Journal}
}

@article{khanhGradient2021,
  title={Convergence rate of a gradient projection method for solving variational inequalities},
  author={Khanh, P. D. and Vuong, P. T. and Vinh, L. V.},
  journal={Journal of Nonlinear and Variational Analysis},
  volume={5},
  number={6},
  pages={951--964},
  year={2021},
  selected={false},
  google_scholar_id={M3ejUd6NZC8C},
  abbr={Journal}
}

@article{khanhFaces2020,
  title={Faces and Support Functions for the Values of Maximal Monotone Operators},
  author={Nguyen, B. T. and Khanh, P. D.},
  journal={Journal of Optimization Theory and Applications},
  volume={186},
  pages={843--863},
  year={2020},
  selected={false},
  google_scholar_id={0EnyYjriUFMC},
  abbr={Journal}
}

@article{khanhLipschitz2020,
  title={Lipschitz Continuity of Convex Functions},
  author={Nguyen, B. T. and Khanh, P. D.},
  journal={Applied Mathematics and Optimization},
  year={2020},
  selected={false},
  google_scholar_id={hqOjcs7Dif8C},
  abbr={Journal}
}

@article{khanhSecondOrder2020,
  title={Second-order characterizations of quasiconvexity and pseudoconvexity for differentiable functions with Lipschitzian derivatives},
  author={Khanh, P. D. and Phat, V. T.},
  journal={Optimization Letters},
  year={2020},
  selected={false},
  google_scholar_id={roLk4NBRz8UC},
  abbr={Journal}
}

@article{khanhExistence2019,
  title={AN ALTERNATIVE PROOF FOR THE SOLUTION EXISTENCE OF FINITE-DIMENSIONAL VARIATIONAL INEQUALITIES},
  author={Khanh, P. D. and Toan, H. P.},
  journal={Linear and Nonlinear Analysis},
  volume={5},
  number={2},
  pages={299--304},
  year={2019},
  selected={false},
  google_scholar_id={5nxA0vEk-isC},
  abbr={Journal}
}

@article{khanhQualitative2019,
  title={Necessary and sufficient conditions for qualitative properties of infinite dimensional linear programming problems},
  author={Khanh, P. D. and Mo, T. H. and Tran, T. T. T.},
  journal={Numerical Functional Analysis and Optimization},
  year={2019},
  selected={false},
  google_scholar_id={UebtZRa9Y70C},
  abbr={Journal}
}

@article{khanhNonsmooth2019,
  title={Characterizations of nonsmooth robustly quasiconvex functions},
  author={Bui, H. T. and Khanh, P. D. and Trinh, T. T. T.},
  journal={Journal of Optimization Theory and Applications},
  volume={180},
  number={3},
  pages={775--786},
  year={2019},
  selected={false},
  google_scholar_id={LkGwnXOMwfcC},
  abbr={Journal}
}

@article{khanhSecondOrder2018,
  title={Second-order characterizations of -smooth robustly quasiconvex functions},
  author={Khanh, P. D. and Phat, V. T.},
  journal={Operations Research Letters},
  volume={46},
  number={6},
  pages={568--572},
  year={2018},
  selected={false},
  google_scholar_id={_FxGoFyzp5QC},
  abbr={Journal}
}

@article{khanhErrorBounds2018,
  title={Error bounds for strongly monotone and Lipschitz continuous variational inequalities},
  author={Khanh, P. D. and Minh, B. N.},
  journal={Optimization Letters},
  volume={12},
  number={5},
  pages={971--984},
  year={2018},
  selected={false},
  google_scholar_id={WF5omc3nYNoC},
  abbr={Journal}
}

@article{khanhConvergence2017,
  title={Convergence rate of a modified extragradient method for pseudomonotone variational inequalities},
  author={Khanh, P. D.},
  journal={Vietnam Journal of Mathematics},
  volume={45},
  number={3},
  pages={397--408},
  year={2017},
  selected={false},
  google_scholar_id={Tyk-4Ss8FVUC},
  abbr={Journal}
}

@article{khanhMordukhovich2017,
  title={The Mordukhovich subdifferentials and directions of descent},
  author={Khanh, P. D. and Yao, J. C. and Yen, N. D.},
  journal={Journal of Optimization Theory and Applications},
  volume={172},
  number={2},
  pages={518--534},
  year={2017},
  selected={false},
  google_scholar_id={UeHWp8X0CEIC},
  abbr={Journal}
}

@article{khanhExtragradient2016,
  title={A new extragradient method for strongly pseudomonotone variational inequalities},
  author={Khanh, P. D.},
  journal={Numerical Functional Analysis and Optimization},
  volume={37},
  number={9},
  pages={1131--1143},
  year={2016},
  selected={false},
  google_scholar_id={2osOgNQ5qMEC},
  abbr={Journal}
}

@article{khanhQuasiconvex2016,
  title={Quasiconvex linear perturbations and convexity},
  author={Khanh, P. D. and Lassonde, M.},
  journal={The American Mathematical Monthly},
  volume={123},
  number={6},
  pages={605--608},
  year={2016},
  selected={false},
  google_scholar_id={Y0pCki6q_DkC},
  abbr={Journal}
}

@article{khanhQualitativeProperties2016,
  title={Qualitative properties of strongly pseudomonotone variational inequalities},
  author={Kim, D. S. and Vuong, P. T. and Khanh, P. D.},
  journal={Optimization Letters},
  volume={10},
  number={8},
  pages={1669--1679},
  year={2016},
  selected={false},
  google_scholar_id={zYLM7Y9cAGgC},
  abbr={Journal}
}

@article{khanhModified2016,
  title={A modified extragradient method for infinite-dimensional variational inequalities},
  author={Khanh, P. D.},
  journal={Acta Mathematica Vietnamica},
  volume={41},
  number={2},
  pages={251--263},
  year={2016},
  selected={false},
  google_scholar_id={IjCSPb-OGe4C},
  abbr={Journal}
}

@article{khanhTikhonov2014,
  title={On the Tikhonov regularization of affine pseudomonotone mappings},
  author={Khanh, P. D.},
  journal={Optimization Letters},
  volume={8},
  pages={1325--1336},
  year={2014},
  selected={false},
  google_scholar_id={u5HHmVD_uO8C},
  abbr={Journal}
}

@article{khanhProjection2014,
  title={Modified projection method for strongly pseudomonotone variational inequalities},
  author={Khanh, P. D. and Vuong, P. T.},
  journal={Journal of Global Optimization},
  volume={58},
  number={2},
  pages={341--350},
  year={2014},
  selected={true},
  google_scholar_id={u-x6o8ySG0sC},
  abbr={Journal}
}

@article{khanhMultivalued2013,
  title={Multivalued Tikhonov trajectories of general affine variational inequalities},
  author={Huong, N. T. T. and Khanh, P. D. and Yen, N. D.},
  journal={Journal of Optimization Theory and Applications},
  volume={158},
  pages={85--96},
  year={2013},
  selected={false},
  google_scholar_id={d1gkVwhDpl0C},
  abbr={Journal}
}
@article{khanhPartial2012,
  title={Partial solution for an open question on pseudomonotone variational inequalities},
  author={Khanh, P. D.},
  journal={Applicable Analysis},
  volume={91},
  number={9},
  pages={1691--1698},
  year={2012},
  selected={false},
  google_scholar_id={9yKSN-GCB0IC},
  abbr={Journal}
}



